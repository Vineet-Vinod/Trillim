{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trillim LoRA Finetuner\n",
    "\n",
    "LoRA fine-tuning for BitNet / Llama / Qwen2 models.\n",
    "\n",
    "**Quick start:**\n",
    "1. Set hyperparameters in the cell below\n",
    "2. Upload your dataset (or modify the dataset loading cell)\n",
    "3. Click **Runtime > Run all**\n",
    "\n",
    "Checkpoints are saved every N epochs to `checkpoints/`.  \n",
    "The final adapter is saved to `finetuned_model/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALL DEPENDENCIES\n",
    "# ============================================================\n",
    "!pip install -q transformers peft bitsandbytes datasets accelerate trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HYPERPARAMETERS & CONFIGURATION\n",
    "# Edit this cell to configure the finetune, then Run All.\n",
    "# ============================================================\n",
    "\n",
    "# --- Model ---\n",
    "MODEL_ID = \"microsoft/bitnet-b1.58-2B-4T-bf16\"\n",
    "\n",
    "# --- Dataset ---\n",
    "# Point this at a directory with your training files, or a single file.\n",
    "# Supported formats: \"text\" (.txt), \"json\" (.json/.jsonl), \"csv\" (.csv)\n",
    "DATASET_DIR = \"dataset\"        # directory containing your data files\n",
    "DATASET_FORMAT = \"text\"         # \"text\", \"json\", or \"csv\"\n",
    "\n",
    "# --- LoRA ---\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"k_proj\", \"q_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# --- Training ---\n",
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "WARMUP_STEPS = 100\n",
    "LOGGING_STEPS = 10\n",
    "OPTIMIZER = \"paged_adamw_8bit\"\n",
    "BF16 = True\n",
    "\n",
    "# --- Checkpointing ---\n",
    "CHECKPOINT_EVERY_N_EPOCHS = 1    # save a checkpoint every N completed epochs\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "OUTPUT_DIR = \"finetuned_model\"\n",
    "\n",
    "# --- Chat template ---\n",
    "# Written into the saved tokenizer so model inference matches training.\n",
    "# Set to \"\" to keep the model's default chat template.\n",
    "CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{{ message['role'] | capitalize }}: {{ message['content'] | trim }}\\n\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}{{ 'Assistant: ' }}{% endif %}\"\n",
    ")\n",
    "\n",
    "# --- Resume from checkpoint (optional) ---\n",
    "# Set to a checkpoint path to resume training, e.g. \"checkpoints/checkpoint-epoch-3\"\n",
    "RESUME_FROM = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATASET LOADING\n",
    "# Modify this cell to parse your dataset as needed.\n",
    "# The only requirement: the resulting `dataset` must have a\n",
    "# \"text\" column with fully-formatted training strings.\n",
    "# ============================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "if DATASET_FORMAT == \"text\":\n",
    "    files = sorted(Path(DATASET_DIR).glob(\"*.txt\"))\n",
    "    assert files, f\"No .txt files found in {DATASET_DIR}/\"\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": [str(f) for f in files]}, split=\"train\")\n",
    "\n",
    "    def format_text(example):\n",
    "        example[\"text\"] = example[\"text\"].replace(\"\\\\n\", \"\\n\")\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(format_text)\n",
    "\n",
    "elif DATASET_FORMAT == \"json\":\n",
    "    files = sorted(Path(DATASET_DIR).glob(\"*.json\")) + sorted(Path(DATASET_DIR).glob(\"*.jsonl\"))\n",
    "    assert files, f\"No .json/.jsonl files found in {DATASET_DIR}/\"\n",
    "    dataset = load_dataset(\"json\", data_files={\"train\": [str(f) for f in files]}, split=\"train\")\n",
    "\n",
    "elif DATASET_FORMAT == \"csv\":\n",
    "    files = sorted(Path(DATASET_DIR).glob(\"*.csv\"))\n",
    "    assert files, f\"No .csv files found in {DATASET_DIR}/\"\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\": [str(f) for f in files]}, split=\"train\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown DATASET_FORMAT: {DATASET_FORMAT!r}\")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} training examples\")\n",
    "print(f\"First example: {repr(dataset[0]['text'][:200])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "try:\n",
    "    from transformers import BitNetQuantConfig\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=BitNetQuantConfig(),\n",
    "    )\n",
    "except ImportError:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE LoRA\n",
    "# ============================================================\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if RESUME_FROM:\n",
    "    print(f\"Resuming from checkpoint: {RESUME_FROM}\")\n",
    "    model = PeftModel.from_pretrained(model, RESUME_FROM, is_trainable=True)\n",
    "    peft_config = None\n",
    "else:\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=TARGET_MODULES,\n",
    "    )\n",
    "\n",
    "print(\"LoRA configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP TRAINER WITH CHECKPOINT CALLBACK\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "from transformers import TrainerCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "\n",
    "class CheckpointEveryNEpochs(TrainerCallback):\n",
    "    \"\"\"Save LoRA adapter + tokenizer every N completed epochs.\"\"\"\n",
    "\n",
    "    def __init__(self, n, checkpoint_dir, tokenizer):\n",
    "        self.n = max(n, 1)\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
    "        epoch = int(round(state.epoch))\n",
    "        if epoch % self.n == 0:\n",
    "            save_path = self.checkpoint_dir / f\"checkpoint-epoch-{epoch}\"\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_pretrained(str(save_path))\n",
    "            self.tokenizer.save_pretrained(str(save_path))\n",
    "            print(f\"\\n>> Checkpoint saved: {save_path}\\n\")\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    bf16=BF16,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    optim=OPTIMIZER,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    ")\n",
    "\n",
    "checkpoint_callback = CheckpointEveryNEpochs(\n",
    "    n=CHECKPOINT_EVERY_N_EPOCHS,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN\n",
    "# ============================================================\n",
    "print(\"Starting finetuning...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE FINAL MODEL\n",
    "# ============================================================\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "if CHAT_TEMPLATE:\n",
    "    tokenizer.chat_template = CHAT_TEMPLATE\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Final LoRA adapter saved to: {OUTPUT_DIR}/\")\n",
    "print(f\"Checkpoints saved in:        {CHECKPOINT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST THE FINETUNED MODEL (optional)\n",
    "# ============================================================\n",
    "from peft import PeftModel\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "# Reload adapter from the saved output (if not already a PeftModel)\n",
    "if not isinstance(model, PeftModel):\n",
    "    model = PeftModel.from_pretrained(model, OUTPUT_DIR)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "def generate(prompt, max_new_tokens=128, temperature=0.6, top_p=0.9):\n",
    "    text = f\"User: {prompt}\\nAssistant:\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "# Try a test prompt\n",
    "test_prompt = \"Hello, how are you?\"\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Response: {generate(test_prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD (Colab only)\n",
    "# Zip up the final adapter for download.\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "archive_name = \"finetuned_model\"\n",
    "shutil.make_archive(archive_name, \"zip\", OUTPUT_DIR)\n",
    "print(f\"Created {archive_name}.zip\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(f\"{archive_name}.zip\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab â€” zip file is available locally.\")"
   ]
  }
 ]
}